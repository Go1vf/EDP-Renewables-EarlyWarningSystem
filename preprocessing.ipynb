{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"./data/\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Signals Data File\n",
    "signals_training = pd.read_csv(data_path + \"wind-farm-1-signals-training.csv\", sep=\";\")\n",
    "signals_training['Timestamp'] = pd.to_datetime(signals_training['Timestamp'])\n",
    "signals_training = signals_training.groupby(['Turbine_ID', 'Timestamp']).last().reset_index()\n",
    "turbines_id = list(pd.unique(signals_training['Turbine_ID']))\n",
    "\n",
    "# Seperate Systems\n",
    "signal_variables = signals_training.columns.to_list()\n",
    "bearings_variables = list(filter(lambda x: x.startswith(\"Gen_Bear\"), signal_variables))\n",
    "generator_variables = list(filter(lambda x: x.startswith(\"Gen\") and not x.startswith(\"Gen_Bear\"), signal_variables))\n",
    "gearbox_variables = list(filter(lambda x: x.startswith(\"Gear\"), signal_variables))\n",
    "transformer_variables = list(filter(lambda x: x.startswith(\"HVTrafo\"), signal_variables))\n",
    "hydraulic_variables = list(filter(lambda x: x.startswith(\"Hyd\"), signal_variables))\n",
    "\n",
    "# Seperate System Signals\n",
    "bearings = signals_training[['Turbine_ID','Timestamp'] + bearings_variables]\n",
    "generators = signals_training[['Turbine_ID','Timestamp'] + generator_variables]\n",
    "gearbox = signals_training[['Turbine_ID','Timestamp'] + gearbox_variables]\n",
    "transformer = signals_training[['Turbine_ID','Timestamp'] + transformer_variables]\n",
    "hydraulic = signals_training[['Turbine_ID','Timestamp'] + hydraulic_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Weather Data File\n",
    "weather_training = pd.read_csv(data_path + \"wind-farm-1-metmast-training.csv\", sep=\";\")\n",
    "weather_training['Timestamp'] = pd.to_datetime(weather_training['Timestamp'])\n",
    "weather_training = weather_training.groupby(['Timestamp']).first().reset_index()\n",
    "weather_variables = weather_training.columns.to_list()\n",
    "weather_variables.remove('Timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Default Data File\n",
    "failures = pd.read_csv(data_path + \"htw-failures-training.csv\")\n",
    "failures['Timestamp'] = pd.to_datetime(failures['Timestamp'])\n",
    "failures['Timestamp'] = failures['Timestamp'].dt.round('10T')\n",
    "failures['Label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following signal data is missing when defaults happen: T11, 2016-03-03 19:00:00+00:00, GENERATOR\n",
      "The following signal data is missing when defaults happen: T06, 2016-07-11 19:50:00+00:00, GENERATOR\n",
      "The following signal data is missing when defaults happen: T06, 2017-08-19 09:50:00+00:00, HYDRAULIC_GROUP\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    At the exact time of some defaults, no signal values were available\n",
    "    Record the time, turbine and system of defaults that will be missing after merge\n",
    "\"\"\"\n",
    "\n",
    "signal_dic = {\n",
    "    'GENERATOR': generators,\n",
    "    'HYDRAULIC_GROUP': hydraulic,\n",
    "    'GENERATOR_BEARING': bearings,\n",
    "    'TRANSFORMER': transformer,\n",
    "    'GEARBOX': gearbox\n",
    "}\n",
    "\n",
    "missing_signals = {\n",
    "    'GENERATOR': [],\n",
    "    'HYDRAULIC_GROUP': [],\n",
    "    'GENERATOR_BEARING': [],\n",
    "    'TRANSFORMER': [],\n",
    "    'GEARBOX': []\n",
    "}\n",
    "\n",
    "for _, row in failures.iterrows():\n",
    "    idx, time, comp = row['Turbine_ID'], row['Timestamp'], row['Component']\n",
    "    data = signal_dic[comp]\n",
    "    data = data[(data['Turbine_ID'] == idx) & (data['Timestamp'] == time)]\n",
    "    if(len(data) == 0):\n",
    "        print(f'The following signal data is missing when defaults happen: {idx}, {time}, {comp}')\n",
    "        missing_signals[comp].append(pd.DataFrame({'Turbine_ID': idx, 'Timestamp': time, 'Label': 1}, index=[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the lead time of default\n",
    "def get_Lead_Time(group):\n",
    "    group['Next_Default_Date'] = group['Timestamp'].where(group['Label'] == 1).fillna(method='bfill')\n",
    "    \n",
    "    # Fillna with a infinite future date when no record of default\n",
    "    infinite_future_date = pd.to_datetime('2262-04-11 00:00:00')\n",
    "    infinite_future_date = infinite_future_date.tz_localize('UTC')\n",
    "    group['Next_Default_Date'].fillna(infinite_future_date, inplace=True)\n",
    "\n",
    "    group['Lead_Time'] = (group['Next_Default_Date'] - group['Timestamp']).dt.days\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Combine the signals, weather and default data\n",
    "    Append the missing data recorded\n",
    "    Fillna using forward fill\n",
    "    Calculate lead time and whether there is default in 60 days\n",
    "\"\"\"\n",
    "\n",
    "processed = {}\n",
    "\n",
    "for comp, data in signal_dic.items():\n",
    "    temp = pd.merge(\n",
    "        data, \n",
    "        weather_training, \n",
    "        how='left', \n",
    "        on='Timestamp'\n",
    "        )\n",
    "    temp = pd.merge(\n",
    "        temp, \n",
    "        failures[failures['Component'] == comp][['Turbine_ID','Timestamp','Label']], \n",
    "        how='left', \n",
    "        on=['Turbine_ID','Timestamp']\n",
    "        )\n",
    "    for line in missing_signals[comp]:\n",
    "        temp = pd.concat([temp, line])\n",
    "    temp.sort_values(by=['Turbine_ID', 'Timestamp'], inplace=True)\n",
    "    temp.Label.fillna(0, inplace=True)\n",
    "    temp = temp.groupby('Turbine_ID').apply(lambda x: x.fillna(method='ffill')).reset_index(drop=True)\n",
    "    temp = temp.groupby('Turbine_ID').apply(get_Lead_Time).reset_index(drop=True)\n",
    "    temp['Default_in_60'] = temp['Lead_Time'] <= 60\n",
    "    processed[comp] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_last_negative(group):\n",
    "    group_sorted = group.sort_values(by='Timestamp', ascending=False).reset_index(drop=True)\n",
    "    last_timestamp = group_sorted.iloc[0, 1]\n",
    "    idx_to_cut = None\n",
    "    for idx, row in group_sorted.iterrows():\n",
    "        curr_time, will_default = row['Timestamp'], row['Default_in_60']\n",
    "        if will_default:\n",
    "            idx_to_cut = idx\n",
    "            break\n",
    "        elif (last_timestamp - curr_time).days >= 60:\n",
    "            idx_to_cut = idx\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    return group_sorted.iloc[idx_to_cut:, :].sort_values(by='Timestamp').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for comp, data in processed.items():\n",
    "    processed[comp] = data.groupby('Turbine_ID').apply(lambda group: remove_last_negative(group)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that no NaN is in processed data\n",
    "for comp, data in processed.items():\n",
    "    assert(data.isna().sum().sum() == 0)\n",
    "\n",
    "# Save data files\n",
    "save_data_path = \"./data/\"\n",
    "for comp, data in processed.items():\n",
    "    data.to_csv(save_data_path+comp+\"_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Under the assumption that subsystems might not be independent of each other\n",
    "    Signal data from one subsystem might be helpful in detecting defaults in another system\n",
    "    We thus create an all signals data file that combines all data\n",
    "\"\"\"\n",
    "all_time = pd.DataFrame()\n",
    "for comp, data in processed.items():\n",
    "    all_time = pd.concat([all_time, data[['Turbine_ID','Timestamp']]]).drop_duplicates()\n",
    "all_time = pd.concat([all_time, failures[['Turbine_ID','Timestamp']]]).drop_duplicates()\n",
    "\n",
    "temp = processed['GENERATOR']\n",
    "temp_var = temp.columns.to_list()\n",
    "temp_var[-4:] = [v + \"_GENERATOR\" for v in temp_var[-4:]]\n",
    "temp.columns = temp_var\n",
    "\n",
    "temp = pd.merge(all_time, temp, how=\"left\", on=['Turbine_ID','Timestamp'])\n",
    "\n",
    "for key in ['HYDRAULIC_GROUP', 'GENERATOR_BEARING', 'TRANSFORMER', 'GEARBOX']:\n",
    "    df = processed[key]\n",
    "    df_var = df.columns.to_list()\n",
    "    df_var[-4:] = [v + f\"_{key}\" for v in df_var[-4:]]\n",
    "    df.columns = df_var\n",
    "    df = pd.merge(all_time, df, how=\"left\", on=['Turbine_ID','Timestamp']).drop(columns=weather_variables)\n",
    "    temp = pd.merge(temp, df, how=\"left\", on=['Turbine_ID','Timestamp'])\n",
    "\n",
    "feat = temp.columns.to_list()\n",
    "filtered_feat = [f for f in feat if f not in signal_variables and f not in weather_variables]\n",
    "temp = temp[['Turbine_ID', 'Timestamp'] + generator_variables + hydraulic_variables + bearings_variables + \\\n",
    "            transformer_variables + gearbox_variables + weather_variables + filtered_feat]\n",
    "\n",
    "def fill_next_default_nan(group):\n",
    "    group.sort_values(by='Timestamp', inplace=True)\n",
    "    for label in ['GENERATOR', 'HYDRAULIC_GROUP', 'GENERATOR_BEARING', 'TRANSFORMER', 'GEARBOX']:\n",
    "        group['Label_' + label] = group['Label_' + label].fillna(0)\n",
    "        group['Next_Default_Date_' + label] = group['Next_Default_Date_' + label].fillna(method='bfill')\n",
    "        group['Lead_Time_' + label] = (group['Next_Default_Date_' + label] - group['Timestamp']).dt.days\n",
    "        group['Default_in_60_' + label] = group['Lead_Time_' + label] <= 60\n",
    "    return group\n",
    "\n",
    "temp = temp.groupby('Turbine_ID').apply(fill_next_default_nan).reset_index(drop=True)\n",
    "temp = temp.sort_values(by=['Turbine_ID', 'Timestamp']).groupby('Turbine_ID').apply(lambda x: x.fillna(method='ffill')).reset_index(drop=True)\n",
    "\n",
    "assert(temp.isna().sum().sum() == 0)\n",
    "temp.to_csv(\"./data/all_signals_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in ['GENERATOR', 'HYDRAULIC_GROUP', 'GENERATOR_BEARING', 'TRANSFORMER', 'GEARBOX']:\n",
    "    assert(sum(temp[f'Label_{label}']) == sum(failures['Component'] == label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['Date'] = temp['Timestamp'].dt.date\n",
    "temp['Hour'] = temp['Timestamp'].dt.hour\n",
    "temp['Date'] = pd.to_datetime(temp['Date'])\n",
    "temp['Timestamp'] = temp.apply(\n",
    "    lambda row: pd.Timestamp(\n",
    "        year=row['Date'].year, \n",
    "        month=row['Date'].month, \n",
    "        day=row['Date'].day, \n",
    "        hour=row['Hour']), \n",
    "    axis=1)\n",
    "temp['Timestamp'] = pd.to_datetime(temp['Timestamp'])\n",
    "temp.drop(columns=['Date', 'Hour'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_mean = temp.iloc[:, :-20].groupby(by=[\"Turbine_ID\", \"Timestamp\"]).mean().reset_index()\n",
    "label_max = pd.concat([temp.iloc[:, :2], temp.iloc[:, -20:]], axis=1).groupby(by=[\"Turbine_ID\", \"Timestamp\"]).max().reset_index()\n",
    "df_hourly = pd.merge(feat_mean, label_max, how=\"left\", on=[\"Turbine_ID\", \"Timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Lead_Time_hourly(group):\n",
    "    for system in ['GENERATOR', 'HYDRAULIC_GROUP', 'GENERATOR_BEARING', 'TRANSFORMER', 'GEARBOX']:\n",
    "        group[f'Next_Default_Date_{system}'] = group['Timestamp'].where(group[f'Label_{system}'] == 1).fillna(method='bfill')\n",
    "    \n",
    "        # Fillna with a infinite future date when no record of default\n",
    "        infinite_future_date = pd.to_datetime('2262-04-11 00:00:00')\n",
    "        group[f'Next_Default_Date_{system}'].fillna(infinite_future_date, inplace=True)\n",
    "\n",
    "        group[f'Lead_Time_{system}'] = (group[f'Next_Default_Date_{system}'] - group['Timestamp']).dt.days\n",
    "        group[f'Default_in_60_{system}'] = group[f'Lead_Time_{system}'] <= 60\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly = df_hourly.groupby(by=\"Turbine_ID\").apply(get_Lead_Time_hourly).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(df_hourly.isna().sum().sum() == 0)\n",
    "temp.to_csv(\"./data/all_signals_hourly_processed.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
